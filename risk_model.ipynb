{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.715950Z",
     "start_time": "2025-05-08T04:25:58.707609Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.738828Z",
     "start_time": "2025-05-08T04:25:58.725568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "ff499fba4c7b7b39",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.818843Z",
     "start_time": "2025-05-08T04:25:58.788398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Loading and Preparation ---\n",
    "df = pd.read_csv(\"combined_stock_data_wide.csv\", parse_dates=[\"Date\"])\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Identify close price and volume columns\n",
    "close_cols = [c for c in df.columns if c.endswith(\"_Close\") and \"Adj\" not in c]\n",
    "volume_cols = [c for c in df.columns if c.endswith(\"_Volume\")]\n",
    "asset_count = len(close_cols)"
   ],
   "id": "22271dfad691ef6d",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.824208Z",
     "start_time": "2025-05-08T04:25:58.819347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "horizon = 7  # Prediction horizon (days)\n",
    "sequence_length = 35  # Input sequence length\n",
    "initial_train_frac = 0.80  # Initial training data fraction\n",
    "batch_size = 64  # Batch size for training\n",
    "num_epochs = 50  # Max epochs per retraining\n",
    "patience = 3  # Early stopping patience\n",
    "learning_rate = 3e-4  # Optimizer learning rate\n",
    "hidden_size = 128  # LSTM hidden units\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "dropout_prob = 0.2  # Dropout probability\n",
    "mc_samples = 90  # Monte Carlo samples for uncertainty\n",
    "lambda_dir = 0.6  # Directional loss weight"
   ],
   "id": "1cbdd264eae33752",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.833829Z",
     "start_time": "2025-05-08T04:25:58.824208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute log returns and realized volatility\n",
    "close_df = df[close_cols]\n",
    "log_returns = np.log(close_df / close_df.shift(1))\n",
    "realized_vol = log_returns.rolling(window=horizon, min_periods=horizon).std().shift(-horizon)"
   ],
   "id": "269403b2770b752d",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.842037Z",
     "start_time": "2025-05-08T04:25:58.833829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature engineering\n",
    "past_std_ewm = log_returns.ewm(span=horizon, adjust=False).std().add_suffix(\"_ewm\")\n",
    "disc_log_ret = log_returns.add_suffix(\"_logret\")\n",
    "liq = (np.log1p(df[volume_cols]).diff().ewm(span=horizon, adjust=False).mean()\n",
    "       .rename(columns=lambda c: c.replace(\"_Volume\", \"_liq\")))"
   ],
   "id": "1c368e72480cf3ce",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.861270Z",
     "start_time": "2025-05-08T04:25:58.849544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Align data by date\n",
    "common_idx = (past_std_ewm.index.intersection(disc_log_ret.index)\n",
    "              .intersection(liq.index).intersection(realized_vol.index))\n",
    "past_std_ewm = past_std_ewm.loc[common_idx]\n",
    "disc_log_ret = disc_log_ret.loc[common_idx]\n",
    "liq = liq.loc[common_idx]\n",
    "realized_vol = realized_vol.loc[common_idx]\n",
    "log_returns = log_returns.loc[common_idx]\n",
    "\n",
    "# Combine features and targets\n",
    "features = pd.concat([past_std_ewm, disc_log_ret, liq], axis=1)\n",
    "targets = realized_vol.copy()\n",
    "combined = pd.concat([features, targets], axis=1).dropna()\n",
    "features = combined[features.columns].reset_index(drop=True)\n",
    "targets = combined[targets.columns].reset_index(drop=True)"
   ],
   "id": "19fb36575a52d547",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.885745Z",
     "start_time": "2025-05-08T04:25:58.865275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create sequences for LSTM\n",
    "def create_sequences(X_df, Y_df, seq_len):\n",
    "    Xs, Ys = [], []\n",
    "    for i in range(len(X_df) - seq_len):\n",
    "        Xs.append(X_df.iloc[i:i + seq_len].values)\n",
    "        Ys.append(Y_df.iloc[i + seq_len - 1].values)\n",
    "    return np.array(Xs), np.array(Ys)\n",
    "\n",
    "X_all, y_all = create_sequences(features, targets, sequence_length)\n",
    "N = len(X_all)\n",
    "assert N > 0, \"No sequences generated!\"\n"
   ],
   "id": "262e4649d2c50fbb",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.916377Z",
     "start_time": "2025-05-08T04:25:58.900060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- LSTM Model with Monte Carlo Dropout ---\n",
    "class VolLSTM_MC(nn.Module):\n",
    "    def __init__(self, in_feats, hid, nlayers, out_feats, drop):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_feats, hid, nlayers, batch_first=True, dropout=drop)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.fc = nn.Linear(hid, out_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        h = self.dropout(h)\n",
    "        h = torch.relu(h)\n",
    "        return self.fc(h)\n",
    "\n",
    "# Initialize model\n",
    "model = VolLSTM_MC(X_all.shape[2], hidden_size, num_layers, asset_count, dropout_prob).to(device)"
   ],
   "id": "70cfbf892ef6e28f",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:25:58.940329Z",
     "start_time": "2025-05-08T04:25:58.930380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Walk-Forward Simulation ---\n",
    "initial_train_size = int(initial_train_frac * N)\n",
    "n_steps = N - initial_train_size - horizon\n",
    "results = []\n",
    "\n",
    "for t in range(n_steps):\n",
    "    print(f\"Step {t + 1}/{n_steps}: Predicting day {initial_train_size + t + horizon}\")\n",
    "\n",
    "    set_seed(42 + t)  # Vary seed for MC dropout\n",
    "\n",
    "    train_end_idx = initial_train_size + t\n",
    "    pred_idx = train_end_idx\n",
    "\n",
    "    # Training data\n",
    "    X_train = X_all[:train_end_idx]\n",
    "    y_train = y_all[:train_end_idx]\n",
    "\n",
    "    # Scale data\n",
    "    scaler_X = StandardScaler().fit(X_train.reshape(-1, X_all.shape[2]))\n",
    "    scaler_y = StandardScaler().fit(y_train)\n",
    "    X_train_scaled = scaler_X.transform(X_train.reshape(-1, X_all.shape[2])).reshape(-1, sequence_length,\n",
    "                                                                                     X_all.shape[2])\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train_scaled, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Optimizer and early stopping\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_loss = float('inf')\n",
    "    wait = 0\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            mse_loss = nn.MSELoss()(y_pred, y_batch)\n",
    "            true_dir = torch.sign(y_batch[:, 1:] - y_batch[:, :-1])\n",
    "            pred_dir = torch.sign(y_pred[:, 1:] - y_batch[:, :-1])\n",
    "            bce_loss = nn.BCEWithLogitsLoss()(pred_dir, (true_dir > 0).float())\n",
    "            loss = mse_loss + lambda_dir * bce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            wait = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Prediction with uncertainty\n",
    "    X_pred = X_all[pred_idx:pred_idx + 1]\n",
    "    y_true = y_all[pred_idx:pred_idx + 1]\n",
    "    X_pred_scaled = scaler_X.transform(X_pred.reshape(-1, X_all.shape[2])).reshape(1, sequence_length, X_all.shape[2])\n",
    "\n",
    "    def enable_dropout(m):\n",
    "        for layer in m.modules():\n",
    "            if isinstance(layer, nn.Dropout):\n",
    "                layer.train()\n",
    "\n",
    "    model.eval()\n",
    "    enable_dropout(model)\n",
    "\n",
    "    mc_predictions = []\n",
    "    with torch.no_grad():\n",
    "        X_pred_tensor = torch.tensor(X_pred_scaled, dtype=torch.float32).to(device)\n",
    "        for _ in range(mc_samples):\n",
    "            mc_predictions.append(model(X_pred_tensor).cpu().numpy())\n",
    "    mc_predictions = np.stack(mc_predictions, axis=0)\n",
    "\n",
    "    # Back to pre-scaled\n",
    "    y_pred_mean = scaler_y.inverse_transform(mc_predictions.mean(axis=0))\n",
    "    y_pred_std = mc_predictions.std(axis=0) * scaler_y.scale_\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred_mean, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred_mean, multioutput='raw_values'))\n",
    "\n",
    "    # Directional accuracy\n",
    "    dir_acc = None\n",
    "    if pred_idx >= horizon:\n",
    "        y_true_current = y_all[pred_idx - horizon]\n",
    "        y_true_future = y_true[0]\n",
    "        pred_direction = np.sign(y_pred_mean[0] - y_true_current)\n",
    "        true_direction = np.sign(y_true_future - y_true_current)\n",
    "        dir_acc = (true_direction == pred_direction).astype(float)\n",
    "        \n",
    "    # Uncertainty-based portfolio allocation\n",
    "    weights = 1 / (y_pred_std[0] + 1e-6)\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    # Portfolio volatility with corrected correlation matrix\n",
    "    # Step 1: compute daily log returns (already defined earlier)\n",
    "    #    log_returns = np.log(close_df / close_df.shift(1))\n",
    "    \n",
    "    # Step 2: aggregate into 7-day log returns\n",
    "    log7 = log_returns.rolling(window=horizon).sum().dropna()\n",
    "    \n",
    "    # Step 3: restrict to the data up to the prediction point\n",
    "    log7_window = log7.iloc[: pred_idx + sequence_length]\n",
    "    \n",
    "    # Step 4: convert those 7-day log returns to 7-day simple returns\n",
    "    window_data = np.exp(log7_window) - 1\n",
    "    \n",
    "    # Step 5: compute exponentially weighted covariance on 7-day simple returns\n",
    "    ewm_cov = window_data.ewm(span=horizon, adjust=False).cov()\n",
    "    \n",
    "    # Step 6: extract the latest 7-day covariance matrix\n",
    "    last_timestamp = window_data.index[-1]\n",
    "    cov_matrix = (\n",
    "        ewm_cov\n",
    "          .loc[last_timestamp]\n",
    "          .values\n",
    "          .reshape(asset_count, asset_count)\n",
    "    )\n",
    "    \n",
    "    # Step 7: build correlation matrix\n",
    "    variances = np.diag(cov_matrix)\n",
    "    stds      = np.sqrt(variances)\n",
    "    corr_matrix = cov_matrix / np.outer(stds, stds)\n",
    "    corr_matrix = np.nan_to_num(corr_matrix, nan=0, posinf=1, neginf=-1)\n",
    "    corr_matrix = np.clip(corr_matrix, -1, 1)\n",
    "    \n",
    "    # Step 8: reconstruct predicted and true covariance in simple-return space\n",
    "    D_pred   = np.diag(y_pred_mean[0])  # predicted 7-day simple-return stds\n",
    "    cov_pred = D_pred @ corr_matrix @ D_pred\n",
    "    vol_pred_port = np.sqrt(weights @ cov_pred @ weights)\n",
    "    \n",
    "    D_true   = np.diag(y_true[0])       # actual 7-day simple-return stds\n",
    "    cov_true = D_true @ corr_matrix @ D_true\n",
    "    vol_true_port = np.sqrt(weights @ cov_true @ weights)\n",
    "\n",
    "    \n",
    "    # Store results with covariance\n",
    "    results.append({\n",
    "        'step': t,\n",
    "        'date': log_returns.index[pred_idx + sequence_length - 1],\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'dir_acc': dir_acc,\n",
    "        'vol_pred_port': vol_pred_port,\n",
    "        'vol_true_port': vol_true_port,\n",
    "        'y_pred_std': y_pred_std,\n",
    "        'weights': weights,\n",
    "        'cov_pred': cov_pred  # Added covariance matrix\n",
    "    })"
   ],
   "id": "e90fd58b2f7d4551",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realized_vol rows: 802\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Summary and Visualizations ---\n",
    "print(\"\\nWalk-Forward Validation Summary:\")\n",
    "mae_all = np.array([res['mae'] for res in results])\n",
    "rmse_all = np.array([res['rmse'] for res in results])\n",
    "dir_acc_all = np.array([res['dir_acc'] for res in results if res['dir_acc'] is not None])\n",
    "vol_pred_port_all = [res['vol_pred_port'] for res in results]\n",
    "vol_true_port_all = [res['vol_true_port'] for res in results]"
   ],
   "id": "d6c3f1f00494ecce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Average metrics\n",
    "mae_mean = mae_all.mean(axis=0)\n",
    "mae_std = mae_all.std(axis=0)\n",
    "rmse_mean = rmse_all.mean(axis=0)\n",
    "rmse_std = rmse_all.std(axis=0)\n",
    "dir_acc_mean = dir_acc_all.mean(axis=0)\n",
    "\n",
    "# Compute standard error for DirAcc with Binomial approximation\n",
    "n_steps_dir_acc = len(dir_acc_all)\n",
    "dir_acc_se = np.sqrt(dir_acc_mean * (1 - dir_acc_mean) / n_steps_dir_acc)\n",
    "\n",
    "# Convert to percentage for reporting\n",
    "dir_acc_mean = dir_acc_mean * 100\n",
    "dir_acc_se = dir_acc_se * 100\n",
    "\n",
    "# Portfolio volatility\n",
    "vol_pred_port_mean = np.mean(vol_pred_port_all)\n",
    "vol_true_port_mean = np.mean(vol_true_port_all)\n",
    "\n",
    "print(\"\\nOverall Performance Across Assets:\")\n",
    "for i, col in enumerate(close_cols):\n",
    "    print(f\"{col}: MAE={mae_mean[i]:.4f} ± {mae_std[i]:.4f}, \"\n",
    "          f\"RMSE={rmse_mean[i]:.4f} ± {rmse_std[i]:.4f}, \"\n",
    "          f\"DirAcc={dir_acc_mean[i]:.1f}% ± {dir_acc_se[i]:.1f}%\")\n",
    "print(f\"\\nPortfolio Volatility: Predicted={vol_pred_port_mean:.4f}, True={vol_true_port_mean:.4f}\")"
   ],
   "id": "e6a1eb6bae2f4ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualizations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([r['y_pred_std'].mean() for r in results], label='Mean Prediction Std Dev', marker='.')\n",
    "plt.title('Prediction Uncertainty Over Time')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Std Dev')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(vol_pred_port_all, label='Predicted Portfolio Vol', marker='.')\n",
    "plt.plot(vol_true_port_all, label='True Portfolio Vol', marker='.')\n",
    "plt.title('Predicted vs True Portfolio Volatility')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Volatility')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f5a627aebc8dac12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save results without removing y_pred_std\n",
    "with open('risk_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump({'results': results, 'close_cols': close_cols, 'log_returns': log_returns}, f)\n",
    "print(\"Risk model results saved to 'risk_model_results.pkl'\")"
   ],
   "id": "9b4998e8126f2525"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
