{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle "
   ],
   "id": "155058ad657ad10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Configuration ---\n",
    "horizon      = 7\n",
    "k            = 6\n",
    "initial_frac = 0.80\n",
    "seed         = 42\n",
    "\n",
    "mc_samples   = 90     # MC dropout passes\n",
    "max_epochs   = 50     # max epochs per step\n",
    "lr           = 3e-4\n",
    "bce_weight   = 1.1    # weight for direction loss\n",
    "dropout_p    = 0.3\n",
    "es_patience  = 4      # early-stop patience"
   ],
   "id": "8d70d0ca3eec92c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Feature toggles ---\n",
    "use_rsi   = True    # include RSI\n",
    "use_macd  = False    # include MACD\n",
    "use_dow   = True    # include day-of-week dummies\n",
    "# --- Seed everything ---\n",
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
    "set_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "6c9d7be424b26454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Load & prepare data ---\n",
    "df = (\n",
    "    pd.read_csv(\"combined_stock_data_wide.csv\", parse_dates=[\"Date\"])\n",
    "      .set_index(\"Date\")\n",
    "      .dropna()\n",
    ")\n",
    "close_cols = [c for c in df.columns if c.endswith(\"_Close\") and \"Adj\" not in c]\n",
    "vol_cols   = [c for c in df.columns if c.endswith(\"_Volume\")]\n",
    "n_assets   = len(close_cols)"
   ],
   "id": "5e2c9147f132cd36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Compute returns & horizon target ---\n",
    "df_ret = np.log(df[close_cols] / df[close_cols].shift(1))\n",
    "ret_h  = df_ret.rolling(horizon).sum().shift(-horizon)\n",
    "\n",
    "# --- Feature engineering ---\n",
    "\n",
    "# Basic price features\n",
    "mom    = df_ret.rolling(k).mean().shift(1)\n",
    "ewm5   = df_ret.ewm(span=5, adjust=False).mean().shift(1)\n",
    "vol    = df_ret.rolling(k).std().shift(1)\n",
    "zscore = (df_ret - df_ret.rolling(k).mean()) / (vol + 1e-8)\n",
    "mr     = -np.sign(zscore)\n",
    "\n",
    "# Liquidity\n",
    "liq = (\n",
    "    np.log1p(df[vol_cols])\n",
    "      .diff()\n",
    "      .ewm(span=horizon, adjust=False)\n",
    "      .mean()\n",
    "      .shift(1)\n",
    ")\n",
    "liq.columns = [c.replace(\"_Volume\", \"_liq\") for c in liq.columns]\n",
    "\n",
    "# Day-of-week\n",
    "if use_dow:\n",
    "    dow = pd.get_dummies(df.index.dayofweek, prefix=\"dow\").set_index(df.index)\n",
    "else:\n",
    "    dow = pd.DataFrame(index=df.index)\n",
    "\n",
    "# RSI\n",
    "if use_rsi:\n",
    "    rsi_list = []\n",
    "    window = 6\n",
    "    for col in close_cols:\n",
    "        delta    = df[col].diff()\n",
    "        gain     = delta.clip(lower=0)\n",
    "        loss     = -delta.clip(upper=0)\n",
    "        avg_gain = gain.rolling(window).mean()\n",
    "        avg_loss = loss.rolling(window).mean()\n",
    "        rs       = avg_gain / (avg_loss + 1e-8)\n",
    "        rsi      = 100 - 100 / (1 + rs)\n",
    "        rsi_list.append(rsi.rename(f\"{col}_rsi14\"))\n",
    "    rsi_df = pd.concat(rsi_list, axis=1)\n",
    "else:\n",
    "    rsi_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "# MACD\n",
    "if use_macd:\n",
    "    macd_list, signal_list = [], []\n",
    "    for col in close_cols:\n",
    "        ema12  = df[col].ewm(span=6, adjust=False).mean()\n",
    "        ema26  = df[col].ewm(span=12, adjust=False).mean()\n",
    "        macd   = ema12 - ema26\n",
    "        signal = macd.ewm(span=4, adjust=False).mean()\n",
    "        macd_list.append(macd.rename(f\"{col}_macd\"))\n",
    "        signal_list.append(signal.rename(f\"{col}_macd_sig\"))\n",
    "    macd_df     = pd.concat(macd_list, axis=1)\n",
    "    macd_sig_df = pd.concat(signal_list, axis=1)\n",
    "else:\n",
    "    macd_df     = pd.DataFrame(index=df.index)\n",
    "    macd_sig_df = pd.DataFrame(index=df.index)"
   ],
   "id": "ac2be5cf9c243023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Align & drop NaNs\n",
    "common = mom.index\n",
    "for part in [ewm5, vol, mr, liq, ret_h, dow, rsi_df, macd_df, macd_sig_df]:\n",
    "    common = common.intersection(part.index)\n",
    "\n",
    "X_df = pd.concat(\n",
    "    [mom, ewm5, vol, mr, liq, rsi_df, macd_df, macd_sig_df, dow], axis=1\n",
    ").loc[common].dropna()\n",
    "y_df = ret_h.loc[X_df.index]\n",
    "mask = ~y_df.isna().any(axis=1)\n",
    "X_df, y_df = X_df.loc[mask], y_df.loc[mask]"
   ],
   "id": "afab53e3bc7ab853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train/Test split & scaling\n",
    "T           = X_df.shape[0]\n",
    "train_size  = int(initial_frac * T)\n",
    "steps       = T - train_size\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_df.values[:train_size])\n",
    "X_all    = X_scaler.transform(X_df.values)\n",
    "\n",
    "y_scaler = StandardScaler().fit(y_df.values[:train_size])\n",
    "y_all    = y_scaler.transform(y_df.values)"
   ],
   "id": "ac2f8d744d237839"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Model definition ---\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(self, in_dim, n_assets, hidden=64, p=dropout_p):\n",
    "        super().__init__()\n",
    "        self.shared   = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Dropout(p)\n",
    "        )\n",
    "        self.ret_head = nn.Linear(hidden, n_assets)\n",
    "        self.dir_head = nn.Linear(hidden, n_assets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        return self.ret_head(h), self.dir_head(h)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model     = MultiTaskMLP(X_all.shape[1], n_assets).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ],
   "id": "6535fa13f12bbb4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Storage\n",
    "h_preds_orig, h_stds_orig, dir_accs, true_vals = [], [], [], []\n",
    "step_losses = []\n",
    "\n",
    "# --- Walk-forward loop ---\n",
    "for t in range(steps):\n",
    "    end    = train_size + t\n",
    "    X_tr   = X_all[:end];        y_tr   = y_all[:end]\n",
    "    dir_tr = (y_scaler.inverse_transform(y_tr) > 0).astype(float) # true direction (up = 1, down =0)\n",
    "\n",
    "    X_tr_t = torch.tensor(X_tr, dtype=torch.float32, device=device)\n",
    "    y_tr_t = torch.tensor(y_tr, dtype=torch.float32, device=device)\n",
    "    d_tr_t = torch.tensor(dir_tr, dtype=torch.float32, device=device)\n",
    "\n",
    "    best_loss, patience = float(\"inf\"), 0\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(max_epochs):\n",
    "        model.train(); optimizer.zero_grad()\n",
    "        r_pred, d_logit = model(X_tr_t)\n",
    "        loss = mse_loss(r_pred, y_tr_t) + bce_weight * bce_loss(d_logit, d_tr_t)\n",
    "        val  = loss.item()\n",
    "        if val < best_loss:\n",
    "            best_loss, best_state, patience = val, {k:v.cpu().clone() for k,v in model.state_dict().items()}, 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= es_patience:\n",
    "                break\n",
    "        loss.backward(); optimizer.step()\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    step_losses.append(best_loss)\n",
    "\n",
    "    # Inference\n",
    "    X_te = X_all[end:end+1];  y_te = y_df.values[end]\n",
    "    model.train()\n",
    "    X_te_t = torch.tensor(X_te, dtype=torch.float32, device=device)\n",
    "    ret_samps, dir_samps = [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(mc_samples):\n",
    "            rs, ds = model(X_te_t)\n",
    "            ret_samps.append(rs.cpu().numpy()[0])\n",
    "            dir_samps.append(torch.sigmoid(ds).cpu().numpy()[0])\n",
    "\n",
    "    ret_samps = np.stack(ret_samps)\n",
    "    dir_samps = np.stack(dir_samps)\n",
    "\n",
    "    mean_ret_s = ret_samps.mean(axis=0)\n",
    "    std_ret_s  = ret_samps.std(axis=0)\n",
    "    # Back to pre-scaled\n",
    "    mean_ret_o = y_scaler.inverse_transform(mean_ret_s.reshape(1,-1))[0]\n",
    "    std_ret_o  = std_ret_s * y_scaler.scale_\n",
    "\n",
    "    dir_pred = (dir_samps.mean(axis=0) > 0.5).astype(float)\n",
    "    dir_true = (y_te > 0).astype(float)\n",
    "    dir_acc  = (dir_pred == dir_true).astype(float)\n",
    "\n",
    "    h_preds_orig.append((mean_ret_o, X_df.index[end]))  # Added date as tuple\n",
    "    h_stds_orig.append(std_ret_o)\n",
    "    dir_accs.append(dir_acc)\n",
    "    true_vals.append(y_te)"
   ],
   "id": "dad78b108f950bcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Aggregate & report ---\n",
    "h_preds_arr = np.vstack([p for p, _ in h_preds_orig])  # Unpack predictions\n",
    "h_stds_arr  = np.vstack(h_stds_orig)\n",
    "dir_arr     = np.vstack(dir_accs)\n",
    "\n",
    "print(\"\\nReturn Forecasting w/ MC Dropout Uncertainty:\")\n",
    "for i,col in enumerate(close_cols):\n",
    "    mu, se_mu = h_preds_arr[:,i].mean(), h_preds_arr[:,i].std(ddof=1)/np.sqrt(len(h_preds_arr))\n",
    "    sigma     = h_stds_arr[:,i].mean()\n",
    "    da, se_da = dir_arr[:,i].mean()*100, np.sqrt(dir_arr[:,i].mean()*(1-dir_arr[:,i].mean())/len(dir_arr))*100\n",
    "    print(f\"{col}: E[Return]={mu:.4f}±{se_mu:.4f}, E[Std]={sigma:.4f}, DirAcc={da:.1f}%±{se_da:.1f}%\")\n",
    "\n",
    "# Portfolio summary\n",
    "w     = np.ones(n_assets)/n_assets\n",
    "predp = h_preds_arr.dot(w)\n",
    "truep = np.array(true_vals).dot(w)\n",
    "maep  = mean_absolute_error(truep, predp)\n",
    "se_maep = np.std(np.abs(predp-truep),ddof=1)/np.sqrt(len(predp))\n",
    "rmsep = np.sqrt(mean_squared_error(truep, predp))\n",
    "se_rmsep = np.std((predp-truep)**2,ddof=1)/(2*rmsep*np.sqrt(len(predp)))\n",
    "darp = (dir_arr.dot(w)>0.5).mean()*100\n",
    "se_darp = np.sqrt(darp/100*(1-darp/100)/len(dir_arr))*100\n",
    "avg_unc = h_stds_arr.dot(w).mean()\n",
    "\n",
    "print(\"\\nPortfolio-Level Summary:\")\n",
    "print(f\"MAE={maep:.4f}±{se_maep:.4f}, RMSE={rmsep:.4f}±{se_rmsep:.4f}, \"\n",
    "      f\"DirAcc={darp:.1f}%±{se_darp:.1f}%, AvgUnc={avg_unc:.4f}\")"
   ],
   "id": "5d212c2f3af373ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(predp,  label='Predicted Port Return')\n",
    "plt.plot(truep,  label='Actual Port Return')\n",
    "plt.title(\"Portfolio Return: Predicted vs. Actual\"); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(step_losses, marker='o')\n",
    "plt.title(\"Walk-Forward: Best Training Loss per Step\"); plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.tight_layout(); plt.show()\n"
   ],
   "id": "34b24a38ab1cdc04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save results\n",
    "with open('return_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump({'h_preds_orig': h_preds_orig, 'h_stds_orig': h_stds_orig, 'dir_accs': dir_accs, 'true_vals': true_vals, 'close_cols': close_cols}, f)\n",
    "print(\"Return model results saved to 'return_model_results.pkl'\")"
   ],
   "id": "af7efab8d4e4717a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
